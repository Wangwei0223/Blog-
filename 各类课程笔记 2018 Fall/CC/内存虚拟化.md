引入内存虚拟化技术后，内存系统中存在3种地址。

机器地址（Machine Address，MA）：真实硬件的机器地址，在地址总线上可以见到的地址信号。

虚拟机物理地址（Guest Physical Address，GPA）：经过VMM抽象后虚拟机看到的伪物理地址。

虚拟地址（Virtual Address，VA）：Guest OS提供给其应用程序使用的线性地址空间。




虚拟地址到虚拟机物理地址的映射关系，记作g

虚拟机物理地址到机器地址的映射关系，记作f



操作系统的内存管理单元（Memory Management Unit，MMU）只能完成一次虚拟地址到物理地址的映射，但获得的物理地址只是虚拟机物理地址而不是机器物理地址，所以需要VMM参与，以获得总线上可以使用的机器地址为实现虚拟地址到机器地址的高效转换，目前普遍采用的方法是由VMM根据映射f和g生成复合映射f·g并直接写入MMU，具体的实现方法有两种：



MMU半虚拟化（MMU Paravirtualization）



这种方式主要为Xen所用



主要原理是：当Guest OS创建新页表时，VMM从维护的空闲内存中为其分配页面并进行注册，以后，Guest OS对该页表的写操作都会陷入VMM进行验证和转换；VMM检查页表中的每一项，确保它们只映射到属于该虚拟机的机器页面，而且不包含对页表页面的可写映射；然后，VMM会根据其维护的映射关系f，将页表项中的虚拟机物理地址替换为相应的机器地址；最后把修改过的页表载入MMU，MMU就可以根据修改过的页表直接完成虚拟地址到机器地址的转换。这种方式的本质是将映射关系f·g直接写入Guest OS的页表中，以替换原来的映射g。



影子页表



全虚拟化使用影子页表技术实现内存虚拟化。其与MMU半虚拟化不同的是，VMM为Guest OS的每个页表维护一个影子页表，并将f·g的映射关系写入影子页表，Guest OS的页表内容保持不变，然后，VMM将影子页表写入MMU。






影子页表的维护将带来时间和空间上的较大开销。时间开销主要体现在Guest OS构造页表时不会主动通知 VMM，VMM 必须等到Guest OS发生缺页时才会分析缺页原因再为其补全影子页表。而空间的开销主要体现在VMM需要支持多台虚拟机同时运行，每台虚拟机的 Guest OS 通常会为其上运行的每个进程创建一套页表系统，因此影子页表的空间开销会随着进程数量的增多而迅速增大。



影子页表缓存



为权衡时间开销和空间开销，现在一般采用影子页表缓存（Shadow Page Table Cache）技术，即VMM在内存中维护部分最近使用过的影子页表，只有当影子页表在缓存中找不到时，才构建一个新的影子页表。当前主要的虚拟化技术都采用了影子页表缓存技术。



虚拟机文件系统的虚拟化技术













问题有点大，笼统的试答一下。坑比较深，最好有基本的OS基础才好理解。假设这里的虚拟化是指在一个OS下虚拟化另外一个OS（ESX那种hypervisor直接跑在硬件上的做法，其实大同小异），另外假设这里的虚拟化是指full virtualization而不是Xen那种para-virtualization。最后假设虚拟化的是一个早期的x86机子（没有hardware supported virtualization的存在）。要回答这个问题，首先看为什么一个OS无法直接与另外的OS共存。答案很简单，OS作为硬件上第一层软件，认为自己拥有全部的硬件的访问和控制权，且自己是唯一的控制者。在这种情况下，如果两个OS共存，必然产生问题。OS主要负责管理的是CPU和内存，以及众多的IO设备。于是我们可以分别讨论。hypervisor是实现虚拟化的关键，它会以一个内核态的驱动存在。CPU的虚拟化：背景知识：x86 CPU有一项权限机制，把CPU的状态置于RING 0到RING 3分别使CPU具有最高的权限到最低的权限。以Linux为例，内核运行于RING 0上，而其余全部用户进程运行于RING 3上（Xen比较奇葩，Linux在Xen下面会运行于RING 1）。在用户权限下，所有的IO设备是不可操作的，另外，有些控制寄存寄是无法访问的，一些privilege的指令是不能运行的。因此一个用户进程要想读写文件，进行一些操作，就要依赖于内核。系统调用能够使CPU运行于RING 0，并执行内核代码（具体方法见一些操作系统教程）。背景说完。一个CPU的全部状态其实就是所有寄存器的值，只要保证任何操作之后寄存寄的值在OS看来是正确的，guest OS就可以正常执行。hypervisor会为每个虚拟的CPU创建一个数据结构，模拟CPU的全部寄存器的值，在适当的时候跟踪并修改这些值。那么考虑虚拟化一个CPU，在虚拟化的guest OS里面，CPU无论如何也不可能运行于RING 0，因为这样的话，host OS必然会crash掉。因此，当一个guest OS想要进入到RING 0执行内核代码时，hypervisor会向guest OS说谎，并告诉它，你已经在RING 0上了，而实际上，所有的指令还是在RING 3上。当guest OS访问到任何privilege的东西时，hypervisor会接到fault，此时hypervisor会判断这个指令是什么，并修改相应的虚拟寄存器的状态，然后返回。这样guest OS就可以正常的运行。需要指出的是，在大多数的指令下代码是直接跑在硬件上的，而不需要软件介入。只有在一些权限高的请求下，软件会介入，并维护虚拟的CPU状态。内存的虚拟化：背景知识：虚拟内存，页表结构等。OS的基础内容，不表。hypervisor虚拟化内存的方法是创建一个shadow page table。正常的情况下，一个page table可以用来实现从虚拟内存到物理内存的翻译。在虚拟化的情况下，由于所谓的物理内存仍然是虚拟的，因此shadow page table就要做到：虚拟内存->虚拟的物理内存->真正的物理内存。以下是细节，如果看着闹心，请忽略。hypervisor会维护一个从虚拟内存到物理内存的映射，当guest OS更换自己的page table，也就是改变CR3寄存器的值，hypervisor会因为用户态的权限不足而接到一个general exception，hypervisor会记录用户想要更换的新的页表，并放上一个空的shadow page table，然后返回。这个空的shadow page table会在接下来的执行中造成CPU无法进行地址翻译，而产生page fault。在fault发生后，hypervisor会得到一个虚拟地址，然后根据之前记录的用户的页表结构，翻译出一个虚拟机器地址，然后再把这个虚拟的机器地址，由hypervisor维护的映射翻译为实际的机器地址，然后装入shadow page table，并返回执行。如此，就实现了：虚拟内存->虚拟的物理内存->真正的物理内存。I/O虚拟化：背景知识：memory mapped I/O device。大多数的PCI设备都是直接将自己的某些控制寄存器映射到物理内存空间上，CPU访问这些控制寄存器的方法和访问内存相同。CPU通过修改和读取这些寄存器来操作I/O设备。虚拟化的方法很简单，没当hypervisor接到page fault，并发现实际上虚拟的物理内存地址对应的是一个I/O设备，hypervisor就用软件模拟这个设备的工作情况，并返回。比如当CPU想要写磁盘时，hypervisor就把相应的东西写到一个host OS的文件上，这个文件实际上就模拟了虚拟的磁盘。这里忽略了很多异常处理等等细节，但求简化，可是貌似还是写多了。以上。